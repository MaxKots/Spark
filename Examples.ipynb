{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2c1a85",
   "metadata": {},
   "source": [
    "#### I`ll add more code examples and their results soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914646bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверочный запуск\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as sp\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(\"select 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6eddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark import SparkConf\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "SPARK_CONFIG = [\n",
    "    ('spark.driver.maxResultSize', '4g'),  # Должно быть не более 80% от памяти\n",
    "    ('spark.driver.memory', '4g'), # Память на драйверах\n",
    "    ('spark.executor.memory', '60g'), # Память на исполнителях\n",
    "    ('spark.executor.memoryOverhead', '4g'), # Параметр допустимого переполнения памяти исполнителя\n",
    "    ('spark.driver.memoryOverhead', '4g'), # Параметр допустимого переполнения памяти драйвера\n",
    "    # Если включаешь динамическое добавление ресурсов, то отключи параметр ниже\n",
    "    ('spark.executor.insatances', '5'), # Кол-во исполнителей, стоит учитывать, что драйвер и память будут выделены на инстант\n",
    "    ('spark.executor.cores', '5'), #Было 8. Рекомендовано - 2, обычно ставят 5. Больше ядер - меньше исполнителей, меньше операций ввода-выводла и параллелизма\n",
    "    ('spark.driver.cores', '5'), #Было 8. Рекомендовано - 2, обычно ставят 5. Больше ядер - меньше исполнителей, меньше операций ввода-выводла и параллелизма\n",
    "    ('spark.cores.max', '15'), #Было 16, если ставим число не кратное  кол-ву исполнителей, получим висящие ядра, не участвующие в рабте\n",
    "    ('spark.dynamicAllocation.enabled', 'false'), #Включить динамическое выделение ресурсов\n",
    "    # ('spark.dynamicAllocation.minExecutors', '0'), #Дефолт - 1,то есть исполнители будут подключены автоматически при необходимости\n",
    "    # ('spark.dynamicAllocation.maxExecutors', '24'), #Динамическое распределение исполнителей\n",
    "    # ('spark.dynamicAllocation.executorIdleTimeout', '600s'), #Отключить исполнитель при простое более N-секунд (дефолт - 300)\n",
    "    # ('spark.dynamicAllocation.cachedExecutorIdleTimeout', '600s'), #Отключить исполнитель с кэшем при простое более N-секунд (кэш храниться до падения сессии(дефолт - infinity))\n",
    "    ('spark.sql.adaptive.enabled', 'True'),\n",
    "    ('spark.sql.adaptive.forceApply', 'False'),\n",
    "    ('spark.sql.adaptive.logLevel', 'info'),\n",
    "    ('spark.sql.adaptive.advisoryPartitionSizeInBytes', '256m'),\n",
    "    ('spark.sql.adaptive.coalescePartitions.enabled', 'True'),\n",
    "    ('spark.sql.adaptive.coalescePartitions.minPartitionNum', '1'),\n",
    "    ('spark.sql.adaptive.coalescePartitions.initialPartitionNum', '8192'),\n",
    "    ('spark.sql.adaptive.fetchShuffleBlocksInBatch', 'True'),\n",
    "    ('spark.sql.adaptive.localShuffleReader.enabled', 'True'),\n",
    "    ('spark.sql.adaptive.skewJoin.enabled', 'True'),\n",
    "    ('spark.sql.adaptive.skewJoin.skewedPartitionFactor', '5'),\n",
    "    ('spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes', '400m'),\n",
    "    ('spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin', '0.2'),\n",
    "    ('spark.sql.autoBroadcastJoinThreshold', '-1'),\n",
    "    ('spark.sql.optimizer.dynamicPartitionPrunning.enabled', 'True'),\n",
    "    ('spark.sql.cbo.enabled', 'True'), # Включить Cost-based optimisation в Catalyst \n",
    "    ('spark.scheduler.mode', 'FAIR'), # Наиболее оптимальный алгоритм шедулера. Включать только в том случае, если он настроен\n",
    "    ('spark.sql.codegen', 'false'), #Компилирует каждый запрос в байт-код Java на лету\n",
    "    ('spark.sql.inMemoryColumnarStorage.compressed', 'False'), #Автоматическое сжатие табличного хранилища в памяти\n",
    "    ('spark.sql.inMemoryColumnarStorage.batchsize', '1000'), #Размер пакета кэширования. Слишком большие значения приводят к ошибкам исчерпания памяти\n",
    "    ('spark.sql.parquet.compression.codec', 'snappy'), #Используемый кодек сжатия (uncompressed, snappy, gzip, lzo)\n",
    "    ('spark.sql.broadcastTimeout', 360), #5 минут таймаут ожидания трансляции данных (полезно, если broadcast готов не с самого начала работы запроса, а позднее)\n",
    "    ('spark.sql.execution.arrow.enabled', 'true'), # Для эффективного преобразования датафрейма Spark в Pandas\n",
    "    # ('spark.sql.session.timeZone', 'UTC+3'), #Устанавливаем локальный часовой пояс. Может быть конфликт с данными в Hive\n",
    "    ('spark.bebug.maxToStringFields', '200'), #Максиммальное кол-во записей в строке дебага, все выходящее за кол-во будет скрыто\n",
    "    ('spark.sql.parquet.binaryAsString', 'True'),\n",
    "    ('spark.sql.parquet.int96TimestampConversion', 'True'),\n",
    "    ('spark.sql.parquet.WriteLegacyFormat', 'True'),\n",
    "    ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), #на всякий случай объявляем Kryo как механизм сериализации\n",
    "    ('spark.kryoserializer.buffer', '24m'),\n",
    "    ('spark.kryoserializer.buffer.max', '48m'), \n",
    "    ('spark.sparkContext.setLogLevel', 'OFF'), #- сократить размер логов с варнингами (ERROR\\WARN)\n",
    "    ('spark.sql.shuffle.partitions', '350'), # - Кол-во партиций для DataSet и DataFrame\n",
    "    ('spark.default.parallelism', '350') # - Кол-во партиций для join и reduceByKey для RDD \n",
    "]\n",
    "\n",
    "conf = SparkConf().setAll(SPARK_CONFIG)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba57ee2",
   "metadata": {},
   "source": [
    "#### To show spark general info like version etc. simply type the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb728226",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819a796",
   "metadata": {},
   "source": [
    "#### show Spark-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fff284",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf9c00",
   "metadata": {},
   "source": [
    "#### Show Spark-SQL query explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb927a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from tbl_nm').explain(extended = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845d772",
   "metadata": {},
   "source": [
    "####  Note that you can start a request either through an existing session or create a new one. Both options work pretty well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420efdbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from db_name.tbl_name\").show()\n",
    "# OR\n",
    "spark.newSession().sql(\"select count(*) from db_name.tbl_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823714b1",
   "metadata": {},
   "source": [
    "####  You could also make quries to **Hive** via **HiveSpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "\n",
    "hiveCtx = HiveContext(sc)\n",
    "# print(hiveCtx)\n",
    "rows = hiveCtx.sql(\"select count(*) from db_name.tbl_name\")\n",
    "firstRow = rows.first()\n",
    "\n",
    "firstRow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cdcf6",
   "metadata": {},
   "source": [
    "# Spark basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6ec55",
   "metadata": {},
   "source": [
    "#### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c43937",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [\n",
    "    ['Миша', '111', 'Привет'],\n",
    "    ['Жора', '111', 'Пока'],\n",
    "    ['Глаша', '333', 'Как дела']\n",
    "]\n",
    "df = spark.createDataFrame(ds, ['col1', 'col2', 'col3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab54100",
   "metadata": {},
   "source": [
    "#### NOTE: Take into consideration that if the names of the columns are in <u>Russian</u>, then **df.filter('col1 like \"Жора\"').show()** will <u>NOT</u> work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d15f5",
   "metadata": {},
   "source": [
    "#### To show dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3040df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "# OR\n",
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951d248",
   "metadata": {},
   "source": [
    "#### If you need to truncate DataFreame results  or show results vertically use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b458a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate = 4)\n",
    "#(100, truncate = False)  - show 100 rows and display full of data width, without cropping\n",
    "\n",
    "# You could also show data vertically by:\n",
    "df.show(truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fdfff",
   "metadata": {},
   "source": [
    "#### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083130f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows\n",
    "len(df.columns)\n",
    "\n",
    "# Columns\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f571a9",
   "metadata": {},
   "source": [
    "#### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2769d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('col1').show()\n",
    "\n",
    "# If we want to show all columns \n",
    "df = df.select('*').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f61da",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9078bcc",
   "metadata": {},
   "source": [
    "Documentation: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf60c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "# You can simply port the col function so that you don’t have to write f.col every time\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select('col1', f.concat('col2','col3').alias('конкатенированный')).toPandas()\n",
    "# 'col1' can be replaced with df.columns[0], that is, refer to the column not by name, but by number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e960fe",
   "metadata": {},
   "source": [
    "#### How to drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b77b3f",
   "metadata": {},
   "source": [
    "df.drop('col1', 'col2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1feab1e",
   "metadata": {},
   "source": [
    "#### How to add columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 способ - через withColumn\n",
    "newdf = df.withColumn('Новый столбец', f.substring('col1', 1, 1))\n",
    "newdf.show(1)\n",
    "\n",
    "# 2 способ - через select\n",
    "nefdf = df.select('*', f.substring('col1', 1, 1).alias('Новый столбец'))\n",
    "newdf.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ff570",
   "metadata": {},
   "source": [
    "#### **filter()** and **distinct()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe920d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(f.col('col1').startswith('Жор')).show()\n",
    "# OR\n",
    "df.filter(f.col('col1').like('Жор%')).show()\n",
    "\n",
    "# or HiveQL, remember about the column names in English\n",
    "df.filter('col1 like \"Жор%\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23aa83",
   "metadata": {},
   "source": [
    "#### <u>AND\\OR</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4198d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND\n",
    "df.filter('col1 like \"Г%\"').filter('col1 like \"Ж%\"').show()\n",
    "\n",
    "# OR\n",
    "df.filter('col1 like \"Г%\" or col1 like \"Ж%\"').show()\n",
    "\n",
    "#Стоит запомнить, что побитовое ИЛИ по рангу выше операций сравнения, которые придется заключать в скобки\n",
    "# StructuredAPI, побитовое ИЛИ - это как надо по хорошему\n",
    "(\n",
    "    df\n",
    "    .filter(\n",
    "        f.col('col1').startswith(\"Ж\") | f.col('col1').startswith('Г')\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f15df",
   "metadata": {},
   "source": [
    "#### Multiple 'Like'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60789789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df. \\\n",
    "select(f.col('col_nm1'), f.col('col_nm2')). \\\n",
    "where('col_nm1 like \"%НДС(НЕТ)%\" or col_nm1 like \"%БЕЗ(НДС%\"'). \\\n",
    "show(truncate = False)\n",
    "\n",
    "# OR\n",
    "\n",
    "df. \\\n",
    "select(f.col('col_nm1'), f.col('col_nm2')). \\\n",
    "where(f.col('col_nm1').like('%НДС(НЕТ)%') | f.col('col_nm1').like('%БЕЗ(НДС%')). \\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4db295",
   "metadata": {},
   "source": [
    "#### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('col1').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7440d",
   "metadata": {},
   "source": [
    "#### JOIN and UNION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At firsst lest create 2 DataFrames\n",
    "ctrydf = spark.createDataFrame(\n",
    "    [\n",
    "        [0, \"ru\", \"Russia\"],\n",
    "        [1, \"it\", \"Italy\"],\n",
    "        [2, \"de\", \"Germany\"]\n",
    "    ],[\"id\", \"code\", \"name\"]\n",
    ")\n",
    "ctrydf.show()\n",
    "\n",
    "capitaldf = spark.createDataFrame(\n",
    "        [\n",
    "            [100, 'ru', 'Moscow', 140],\n",
    "            [101, 'it', 'Rome', 100]\n",
    "        ],['id', 'ctry_code', 'city', 'population']\n",
    ")\n",
    "\n",
    "capitaldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e4bc1",
   "metadata": {},
   "source": [
    "#### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use a joinexpression, in which we indicate how the tables will be linked\n",
    "joinExpression = ctrydf['code'] == capitaldf['ctry_code']\n",
    "ctrydf.join(capitaldf, joinExpression, 'inner').show()\n",
    "\n",
    "# otherwise you would have to write it like this:\n",
    "ctrydf.join(capitaldf, ctrydf['code'] == capitaldf['ctry_code'], 'inner').show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
